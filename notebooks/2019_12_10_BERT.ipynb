{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2019-12-10 BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nov05/yelp-dataset-challenge/blob/master/notebooks/2019_12_10_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG44CJAVgaPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# created by nov05 on 2019-12-10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnsFX7ajgiSR",
        "colab_type": "text"
      },
      "source": [
        "https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-dZSDOCguAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjhYcWHQhLt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch_pretrained_bert\n",
        "# Successfully installed pytorch-pretrained-bert-0.6.2 regex-2019.12.9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOIwGsSmheoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1oTHGsXhEcw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e3e221c0-805b-49a0-8035-36988c89f7fe"
      },
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "#logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 2577715.00B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6ktd4oUglYa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "a3cf4ac8-9c74-41a4-e857-3809597c8144"
      },
      "source": [
        "path = \"/content/drive/My Drive/data/2019-12-06 yelp/yelp_dataset/csv_out/tip.csv\"\n",
        "df_tip = pd.read_csv(path)\n",
        "print(df_tip.shape)\n",
        "df_tip.sample(3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1223094, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>business_id</th>\n",
              "      <th>text</th>\n",
              "      <th>date</th>\n",
              "      <th>compliment_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1122599</th>\n",
              "      <td>NkgHjC9Yogw5y42NJ5dSTg</td>\n",
              "      <td>Bh3duLU3aASnt2TZ97TJnQ</td>\n",
              "      <td>Was closed at 8:20 SEPT 27, 2017</td>\n",
              "      <td>2017-09-27 15:23:31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>596378</th>\n",
              "      <td>uegNuvRUU-z4FwFo2aJpcw</td>\n",
              "      <td>Queaai26YT4YWUd_b_6gAg</td>\n",
              "      <td>Love this place. I believe it's the best office.</td>\n",
              "      <td>2013-05-31 15:54:15</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297253</th>\n",
              "      <td>F2IbnY4CpTfSxQijPtDKKg</td>\n",
              "      <td>uGupeWqih0yIcCg8anM1PA</td>\n",
              "      <td>Audition time: show'em how it's done, SON! haha</td>\n",
              "      <td>2010-10-29 05:25:04</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        user_id  ... compliment_count\n",
              "1122599  NkgHjC9Yogw5y42NJ5dSTg  ...                0\n",
              "596378   uegNuvRUU-z4FwFo2aJpcw  ...                0\n",
              "297253   F2IbnY4CpTfSxQijPtDKKg  ...                0\n",
              "\n",
              "[3 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlouSMpuiYKY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fa312e5b-bff0-4604-c0ab-21aff2b7f064"
      },
      "source": [
        "text = df_tip.sample()['text'].values[0]\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Print out the tokens.\n",
        "print (tokenized_text)\n",
        "# the BERT tokenizer was created with a WordPiece model"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'service', 'was', 'awesome', 'drinks', 'were', 'strong', 'and', 'ta', '##sty', '.', 'food', 'was', 'great', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSo10x2Aoo1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "f9dee658-8ab5-4eb3-d9ae-19b6918d0293"
      },
      "source": [
        "# Map the token strings to their vocabulary indeces.\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# Display the words with their indeces.\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS]           101\n",
            "service       2,326\n",
            "was           2,001\n",
            "awesome      12,476\n",
            "drinks        8,974\n",
            "were          2,020\n",
            "strong        2,844\n",
            "and           1,998\n",
            "ta           11,937\n",
            "##sty        21,756\n",
            ".             1,012\n",
            "food          2,833\n",
            "was           2,001\n",
            "great         2,307\n",
            "[SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O12POQG3A3IY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d562f3d4-6e3c-4734-b12a-f0f1c30b7c56"
      },
      "source": [
        "# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
        "segments_ids = [1] * len(tokenized_text)\n",
        "print (segments_ids)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sDYvzoaA8jx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "013815ef-865a-4c1e-b113-ed66e7fe0a30"
      },
      "source": [
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:08<00:00, 49890377.48B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): BertLayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iE1MwaZBY6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict hidden states features for each layer\n",
        "with torch.no_grad():\n",
        "    encoded_layers, _ = model(tokens_tensor, segments_tensors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FYdam8tCpkr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "5980dd7d-ff0e-4c9d-9f79-3dd27aa14869"
      },
      "source": [
        "print (\"Number of layers:\", len(encoded_layers))\n",
        "layer_i = 0\n",
        "print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
        "batch_i = 0\n",
        "print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
        "token_i = 0\n",
        "print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of layers: 12\n",
            "Number of batches: 1\n",
            "Number of tokens: 15\n",
            "Number of hidden units: 768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz_K5QLggW8Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "5e0ffb07-1669-4f44-bd25-88935e1c38e9"
      },
      "source": [
        "# For the 5th token in our sentence, select its feature values from layer 5.\n",
        "token_i = 5\n",
        "layer_i = 5\n",
        "vec = encoded_layers[layer_i][batch_i][token_i]\n",
        "\n",
        "# Plot the values as a histogram to show their distribution.\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.hist(vec, bins=200)\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAAI/CAYAAAC4QOfKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUAAAQ/8fGtDDiDcDLJIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFEnsgEkgndE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "dd5fe449-19c6-4d9e-f727-1c59704e4aa9"
      },
      "source": [
        "# `encoded_layers` is a Python list.\n",
        "print('     Type of encoded_layers: ', type(encoded_layers))\n",
        "\n",
        "# Each layer in the list is a torch tensor.\n",
        "print('Tensor shape for each layer: ', encoded_layers[0].size())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Type of encoded_layers:  <class 'list'>\n",
            "Tensor shape for each layer:  torch.Size([1, 15, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H7mKI3pgs5h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "74a5f5d1-b71b-45f2-f2fb-f8701a3652c0"
      },
      "source": [
        "# Concatenate the tensors for all layers. We use `stack` here to\n",
        "# create a new dimension in the tensor.\n",
        "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
        "token_embeddings.size()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 1, 15, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnbYrF5ypUtN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "044dd420-154e-4200-8e78-213f7c5ff4f8"
      },
      "source": [
        "# Remove dimension 1, the \"batches\".\n",
        "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "token_embeddings.size()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 15, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Css7RwSUt2Ax",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ebcca8a9-0364-4607-a53c-ba6baadcc617"
      },
      "source": [
        "# switch around the “layers” and “tokens” dimensions\n",
        "# Swap dimensions 0 and 1.\n",
        "token_embeddings = token_embeddings.permute(1,0,2)\n",
        "token_embeddings.size()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15, 12, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDor-3F2x8k4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fb5c149b-2ad0-4b5a-cb3d-fa892d2a63ff"
      },
      "source": [
        "# Stores the token vectors, with shape [15 x 3,072]\n",
        "token_vecs_cat = []\n",
        "\n",
        "# `token_embeddings` is a [15 x 12 x 768] tensor.\n",
        "\n",
        "# For each token in the sentence...\n",
        "for token in token_embeddings:\n",
        "    \n",
        "    # `token` is a [12 x 768] tensor\n",
        "\n",
        "    # Concatenate the vectors (that is, append them together) from the last \n",
        "    # four layers.\n",
        "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
        "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
        "    \n",
        "    # Use `cat_vec` to represent `token`.\n",
        "    token_vecs_cat.append(cat_vec)\n",
        "\n",
        "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape is: 15 x 3072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfoi_dJsyF50",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "42d50014-69a9-45dd-f1ee-29b7f4f0a3a1"
      },
      "source": [
        "# Stores the token vectors, with shape [15 x 768]\n",
        "token_vecs_sum = []\n",
        "\n",
        "# `token_embeddings` is a [15 x 12 x 768] tensor.\n",
        "\n",
        "# For each token in the sentence...\n",
        "for token in token_embeddings:\n",
        "\n",
        "    # `token` is a [12 x 768] tensor\n",
        "\n",
        "    # Sum the vectors from the last four layers.\n",
        "    sum_vec = torch.sum(token[-4:], dim=0)\n",
        "    \n",
        "    # Use `sum_vec` to represent `token`.\n",
        "    token_vecs_sum.append(sum_vec)\n",
        "\n",
        "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape is: 15 x 768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht2w4Zgjyg5w",
        "colab_type": "text"
      },
      "source": [
        "# Sentence Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jinFdjRykhF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "915a46a6-868e-4434-c7b1-657da9350253"
      },
      "source": [
        "# `encoded_layers` has shape [12 x 1 x 22 x 768]\n",
        "\n",
        "# `token_vecs` is a tensor with shape [22 x 768]\n",
        "token_vecs = encoded_layers[11][0]\n",
        "\n",
        "# Calculate the average of all 22 token vectors.\n",
        "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "\n",
        "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our final sentence embedding vector of shape: torch.Size([768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiBTJgxNzE7r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "487367f7-693c-4b75-c863-008b638f1bd5"
      },
      "source": [
        "for i, token_str in enumerate(tokenized_text):\n",
        "    print (i, token_str)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [CLS]\n",
            "1 service\n",
            "2 was\n",
            "3 awesome\n",
            "4 drinks\n",
            "5 were\n",
            "6 strong\n",
            "7 and\n",
            "8 ta\n",
            "9 ##sty\n",
            "10 .\n",
            "11 food\n",
            "12 was\n",
            "13 great\n",
            "14 [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fhBRWQKzUXW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "0977fd81-4a58-483a-82d8-55ef077f856f"
      },
      "source": [
        "print('First 5 vector values for 3 adjectives.')\n",
        "print(\"awesome \", str(token_vecs_sum[3][:5]))\n",
        "print(\"strong \", str(token_vecs_sum[6][:5]))\n",
        "print(\"great \", str(token_vecs_sum[13][:5]))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 5 vector values for 3 adjectives.\n",
            "awesome  tensor([3.2793, 0.4395, 0.4706, 3.9851, 3.3688])\n",
            "strong  tensor([-2.8577, -2.6828,  2.8643,  2.7328,  2.1268])\n",
            "great  tensor([-0.0675,  0.7492, -1.2852,  3.2361,  1.3994])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UiZqdR40E-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "81dec116-8b39-441d-9153-cbc3087fd029"
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# difference between \"awesome\" and \"great\" in this sentence\n",
        "diff_1 = 1 - cosine(token_vecs_sum[3], token_vecs_sum[13])\n",
        "# difference between \"awesome\" and \"strong\" in this sentence\n",
        "diff_2 = 1 - cosine(token_vecs_sum[3], token_vecs_sum[6])\n",
        "\n",
        "print('Vector similarity for  *similar*  meanings:  %.2f' % diff_1)\n",
        "print('Vector similarity for *different* meanings:  %.2f' % diff_2)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vector similarity for  *similar*  meanings:  0.84\n",
            "Vector similarity for *different* meanings:  0.55\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}